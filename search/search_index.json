{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"23_11_29_19_34/","text":"Let us consider a system of linear inequalities \\[ Ax\\leq b \\tag{1} \\] where \\(A=(a_{ij})_{i,j=1}^{n,m}\\in\\mathbb{R}^{n\\times m}\\) and \\(b=(b_{i})_{i=1}^{n}\\in\\mathbb{R}^{n}\\) . Farkas' Lemma System (1) is infeasible if and only if there exists $y\\geq 0$ such that $y^{\\top}A=0$ and $y^{\\top}b < 0$. \\(\\rhd\\) If there is a vector \\(y\\geq 0\\) such that \\(y^{\\top}A=0\\) and \\(y^{\\top}b< 0\\) , then obviously system (1) is infeasible. Let us assume that system (1) is infeasible. Using Fourier-Motzkin Elimination we reduce the system until we have no variables. The new system is \\[ (0, 0,\\ldots, 0)^{\\top}\\leq (b_{1}',b_{2}',\\ldots,b_{k}')^{\\top}. \\] Since the new system is also infeasible, it follows that \\(b_{i}'<0\\) for some \\(i\\) . Rewriting and addition steps in the elimination process correspond to row operations on the original matrix \\(A\\) . This is done by matrix multiplication: \\[ 0=SAx\\leq Sb=b' \\] where the matrix \\(S\\) has non-negative entries. Let us denote \\(y=S^{\\top}e_{i}\\) . We have \\(y\\geq 0\\) , \\(y^{\\top}A=0\\) , and \\(y^{\\top}b< 0\\) . \\(\\lhd\\) There is another formulation of Farkas' lemma. Farkas' Lemma (Alternative formulation) A system $Ax=b$, $x\\geq 0$ is infeasible if and only if there exist $y$ such that $y^{\\top}A\\geq 0$ and $y^{\\top}b < 0$. \\(\\rhd\\) Let's consider an equivalent system \\[ \\begin{pmatrix} A\\\\ -A\\\\ -I \\end{pmatrix}x\\leq \\begin{pmatrix} b\\\\ -b\\\\ 0 \\end{pmatrix}. \\] This system is infeasible if and only if there exists a vector \\[ y=\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}\\geq 0 \\] such that \\((y_{1}-y_{2})^{\\top}A=y_{3}^{\\top}\\geq 0\\) and \\((y_{1}-y_{2})^{\\top}b<0\\) . \\(\\lhd\\)","title":"23 11 29 19 34"},{"location":"23_11_29_19_54/","text":"Let \\(A\\in\\mathbb{R}^{n\\times m}\\) , \\(b\\in\\mathbb{R}^{n}\\) , and \\(c\\in\\mathbb{R}^{m}\\) . We consider LP problem (1) \\[ \\begin{array}{rl} \\min& c^{\\top}x\\\\ \\mathrm{s.t.}& Ax\\geq b\\\\ & x\\geq 0 \\end{array} \\tag{1} \\] and its dual problem (2) \\[ \\begin{array}{rl} \\max& b^{\\top}y\\\\ \\mathrm{s.t.}& A^{\\top}y\\leq c\\\\ & y\\geq 0 \\end{array} \\tag{2} \\] Strong Duality Theorem If problem (1) has an optimal solution $x^{\\star}$, then problem (2) also has an optimal solution $y^{\\star}$ and $c^{\\top}x^{\\star}=b^{\\top}y^{\\star}$. \\(\\rhd\\) First we prove that problem (2) has a feasible solution. Assume the opposite; then the system \\[ \\begin{pmatrix} A^{\\top}\\\\ -I \\end{pmatrix}y\\leq \\begin{pmatrix} c\\\\ 0 \\end{pmatrix} \\] is infeasible. From Farkas' lemma , it follows that there exists a non-negative vector \\(\\begin{pmatrix}z_{1}\\\\ z_{2}\\end{pmatrix}\\in\\mathbb{R}_{\\geq 0}^{m+n}\\) such that \\(z_{1}^{\\top}A^{\\top}=z_{2}^{\\top} \\geq 0\\) and \\(z_{1}^{\\top}c<0\\) . Then \\(x^{\\star}+z_{1}\\) is a feasible solution of problem (1) and \\(c^{\\top}(x^{\\star}+z_{1})<c^{\\top}x^{\\star}\\) , which is impossible. Let us note that if \\(x\\) and \\(y\\) are feasible solutions of problems (1) and (2), respectively, then \\(c^{\\top}x\\geq y^{\\top}Ax\\geq b^{\\top}y\\) . Second we prove that problem (2) has an optimal solution. Assume the converse; then the system \\[ \\begin{pmatrix} -b^{\\top}\\\\ A^{\\top}\\\\ -I \\end{pmatrix}y\\leq \\begin{pmatrix} -c^{\\top}x^{\\star}\\\\ c\\\\ 0 \\end{pmatrix} \\] is infeasible. Hence, there is a non-negative vector \\[ \\begin{pmatrix}z_{1}\\\\z_{2}\\\\z_{3}\\end{pmatrix}\\in\\mathbb{R}^{1+m+n}, \\] such that \\(z_{2}^{\\top}A^{\\top}=z_{1}^{\\top}b^{\\top}+z_{3}^{\\top}I\\) and \\(-z_{1}^{\\top}c^{\\top}x^{\\star}+z_{2}^{\\top}c< 0\\) . Denote \\(x'=x^{\\star}+z_{2}\\) if \\(z_{1}=0\\) , and \\(x'=z_{2}/z_{1}\\) otherwise. In the both cases, we obtain that \\(x'\\) is feasible solution of problem (1) and \\(c^{\\top}x'<c^{\\top}x^{\\star}\\) , which is impossible. \\(\\lhd\\)","title":"23 11 29 19 54"},{"location":"23_11_30_18_37/","text":"Suppose we have a system of linear inequalities \\[ Ax\\leq b \\tag{1} \\] where \\(A=(a_{ij})_{i,j=1}^{n,m}\\in\\mathbb{R}^{n\\times m}\\) and \\(b=(b_{i})_{i=1}^{n}\\in\\mathbb{R}^{n}\\) . To eliminate \\(x_{1}\\) from system (1), we modify each inequality \\[ a_{i1}x_{1}+a_{i2}x_{2}+ \\ldots+a_{im}x_{m}\\leq 0 \\] depending on the sign of \\(a_{i1}\\) : \\(x_{1}+\\sum\\limits_{j=2}^{m}\\dfrac{a_{ij}}{a_{i1}}x_{j}\\leq \\dfrac{b_{i}}{a_{i1}}\\) if \\(a_{i1}>0\\) ; \\(-x_{1}+\\sum\\limits_{j=2}^{m}\\dfrac{a_{ij}}{-a_{i1}}x_{j}\\leq \\dfrac{b_{i}}{-a_{i1}}\\) if \\(a_{i1}<0\\) ; \\(\\sum\\limits_{j=2}^{m}a_{ij}x_{j}\\leq b_{i}\\) if \\(a_{i1}=0\\) . Denote \\(\\bar{x}=(x_{2},x_{3},\\ldots,x_{m})^{\\top}\\) . Simplifying the notation, we write each inequality of the first, second, and third types as \\(x_{1}+\\bar{a}_{i_{1}}^{\\top}\\bar{x}\\leq\\bar{b}_{i_{1}}\\) , \\(-x_{1}+\\bar{a}_{i_{2}}^{\\top}\\bar{x}\\leq\\bar{b}_{i_{2}}\\) , and \\(\\bar{a}_{i_{3}}^{\\top}\\bar{x}\\leq\\bar{b}_{i_{3}}\\) respectively. Now we compose a new system with fewer number of variables \\(x_{2}\\) , \\(x_{3}\\) , \\(\\ldots\\) , \\(x_{m}\\) . This system consists of inequalities of the third type and all possible sums of inequalities of the first and second types: \\[ \\begin{cases} \\bar{a}_{i_{3}}^{\\top}\\bar{x}\\leq\\bar{b}_{i_{3}},\\\\ (\\bar{a}_{i_{1}}+\\bar{a}_{i_{2}})^{\\top}\\bar{x}\\leq\\bar{b}_{i_{1}}+\\bar{b}_{i_{2}}. \\end{cases} \\tag{2} \\] Theorem System (1) has a solution if and only if system (2) has a solution. \\(\\rhd\\) One part of the statement is obvious. Indeed, if system (1) has a solution \\(x=(x_{1},x_{2},\\ldots,x_{m})^{\\top}\\) , then \\(\\bar{x}=(x_{2},x_{3}, \\ldots,x_{m})^{\\top}\\) is a solution of system (2). Now, let \\(\\bar{x}=(x_{2},x_{3},\\ldots,x_{m})^{\\top}\\) be a solution of system (2). We have \\[ \\bar{a}_{i_{2}}^{\\top}\\bar{x}-\\bar{b}_{i_{2}}\\leq \\bar{b}_{i_{1}}-\\bar{a}_{i_{1}}^{\\top}\\bar{x}. \\] Hence, to construct a solution of system (1) we can just take an arbitrary \\[ x_{1}\\in[\\max\\limits_{i_{2}}(\\bar{a}_{i_{2}}^{\\top}\\bar{x}-\\bar{b}_{i_{2}}), \\min\\limits_{i_{1}}(\\bar{b}_{i_{1}}-\\bar{a}_{i_{1}}^{\\top}\\bar{x})].\\quad\\lhd \\] The described above variable elimination method is called Fourier-Motzkin Elimination .","title":"23 11 30 18 37"},{"location":"23_12_06_20_09/","text":"Let us apply the Benders decomposition to the following nonlinear problem: \\[ \\begin{array}{rl} \\min& c^{\\top}x+f(y)\\\\ \\mathrm{s.t.}& Ax+g(y)\\geq b\\\\ & x\\geq 0\\\\ & y\\in\\mathbb{Y} \\end{array} \\tag{1} \\] We define an auxiliary function \\(h\\colon\\mathbb{Y}\\to\\mathbb{R}\\) as \\[ \\begin{array}{c} h(y)=\\min\\{c^{\\top}x\\colon Ax\\geq b-g(y),x\\geq0\\}=\\\\ \\max\\{\\bigl(b-g(y)\\bigr)^{\\top}z\\colon A^{\\top}z\\leq c,z\\geq 0\\}. \\end{array} \\] Then problem (1) is equivalent to the following problem \\[ \\begin{array}{rl} \\min& f(y)+h(y)\\\\ \\mathrm{s.t.}& y\\in\\mathbb{Y} \\end{array} \\tag{2} \\] Denote \\(P=\\{z\\colon A^{\\top}z\\leq c,z\\geq 0\\}\\) . According to the decomposition theorem , \\(P=Q+C\\) where \\(Q=\\mathrm{hull}\\{z_{1},z_{2},\\ldots,z_{s}\\}\\) is a polytope and \\(C=\\mathrm{cone}\\{v_{1},v_{2},\\ldots,v_{t}\\}\\) is a finitely generated cone. Then problem (2) is equivalent to the following problem \\[ \\begin{array}{rl} \\min& f(y)+h\\\\ \\mathrm{s.t.}& \\bigl(b-g(y)\\bigr)^{\\top}z_{i}\\leq h,\\;1\\leq i\\leq s\\\\ & \\bigl(b-g(y)\\bigr)^{\\top}v_{j}\\leq 0,\\;1\\leq j\\leq t\\\\ & h\\in\\mathbb{R}\\\\ &y\\in\\mathbb{Y} \\end{array} \\tag{3} \\] Usually there are a lot of \\(z_{i}\\) and \\(v_{j}\\) , so formulation (3) is impractical. Applying the Benders decomposition, we start from the problem \\[ \\begin{array}{rl} \\min& f(y)+h\\\\ \\mathrm{s.t.}& h\\in\\mathbb{R}\\\\ &y\\in\\mathbb{Y} \\end{array} \\tag{4} \\] Let \\((y^{\\star},h^{\\star})\\) be a solution of problem (4). Then we consider the following problem \\[ \\begin{array}{rl} \\max& \\bigl(b-g(y^{\\star})\\bigr)^{\\top}z\\\\ \\mathrm{s.t.}& A^{\\top}z\\leq c\\\\ &z\\geq 0 \\end{array} \\tag{5} \\] If problem (5) is unbounded, then there is \\(v_{j}\\) such that \\(\\bigl(b-g(y^{\\star})\\bigr)^{\\top}v_{j}> 0\\) . We add a new constraint \\(\\bigl(b-g(y)\\bigr)^{\\top}v_{j}\\leq 0\\) to problem (4). If problem (5) has an optimal solution \\(z_{i}\\) , then we have two cases: If \\(h^{\\star}=\\bigl(b-g(y^{\\star})\\bigr)^{\\top}z_{i}\\) , then \\((x^{\\star},y^{\\star})\\) is an optimal solution of problem (1) where \\(x^{\\star}=\\mathrm{argmin}\\{c^{\\top}x\\colon Ax\\geq b-g(y^{\\star}), x\\geq 0\\}\\) . If \\(h^{\\star}<\\bigl(b-g(y^{\\star})\\bigr)^{\\top}z_{i}\\) , then we add a new constraint \\(\\bigl(b-g(y)\\bigr)^{\\top}z_{i}\\leq h\\) to problem (4). If we have not yet received a solution of problem (1), then we solve the updated problem (4) and repeat the steps described above.","title":"23 12 06 20 09"},{"location":"23_12_07_11_50/","text":"Here is a proof of Weyl's theorem, which states that every finitely generated cone is finitely constrained. Let us star with definitions. Definition A cone is a set $C\\subset\\mathbb{R}^{n}$ such that for all vectors $x$, $y\\in C$ and non-negative numbers $\\lambda$, $\\mu\\geq 0$ we have $\\lambda x+\\mu y\\in C$. Definition A finitely constrained cone $C$ is the intersection of finitely many closed linear half-spaces, i.e. $C=\\{x\\colon Ax\\leq 0\\}$ for some real $m\\times n$-matrix $A$. Definition A finitely generated cone $C$ is a set of all linear combinations of finitely many vectors $b_{1}$, $b_{2}$, $\\ldots$, $b_{k}$ with non-negative coefficients, i.e. \\[ \\begin{array}{c} C=\\mathrm{cone}\\,(b_{1},b_{2},\\ldots,b_{k})=\\\\ =\\left\\{\\sum\\limits_{i=1}^{k} \\lambda_{i}b_{i}\\colon\\lambda_{i}\\geq 0\\right\\}=\\{B\\lambda\\colon\\lambda\\geq 0\\}. \\end{array} \\] Weyl's theorem A finitely generated cone is finitely constrained. \\(\\rhd\\) Let \\(C=\\{B\\lambda\\colon\\lambda\\geq 0\\}\\) . A vector \\(x\\) belongs to \\(C\\) if and only if \\(x\\) is the part of a solution of the following system \\[ \\begin{cases} x-B\\lambda\\leq 0,\\\\ -x+B\\lambda\\leq 0,\\\\ -\\lambda\\leq 0. \\end{cases}\\tag{1} \\] Using Fourier-Motzkin elimination to eliminate variables \\(\\lambda_{1}\\) , \\(\\lambda_{2}\\) , \\(\\ldots\\) , \\(\\lambda_{m}\\) from system (1), we obtain a new system of inequalities \\(Ax\\leq 0\\) that defines \\(C\\) . \\(\\lhd\\) Corollary A finitely generated cone is closed. \\(\\rhd\\) Since the intersection of closed half-spaces is closed, it follows that every finitely constrained cone is closed. Finallym, Weyl's theorem implies that every finitely generated cone is closed. \\(\\lhd\\)","title":"23 12 07 11 50"},{"location":"23_12_07_12_08/","text":"Here is a proof of Minkowski's theorem, which states that every finitely constrained cone is finitely generated. This theorem reverses Weyl's theorem . Let us start with a useful definition and lemma. Definition The dual cone $C^{*}$ of a cone $C\\subset\\mathbb{R}^{n}$ is defined as \\[ C^{*}\\stackrel{\\mathrm{def}}{=}\\{y\\in\\mathbb{R}^{n}\\colon y^{\\top}x\\leq 0\\; \\text{for all}\\; x\\in C\\}. \\] It is easy to see that, if \\(C=\\{B\\lambda\\colon \\lambda\\geq0\\}\\) , then \\(C^{*}=\\{y\\colon B^{\\top}y\\leq 0\\}\\) . Lemma If $C$ is a closed cone, then $C^{**}=C$. \\(\\rhd\\) We obviously have \\(C\\subset C^{**}\\) . Suppose that there exists \\(z\\in C^{**}\\setminus C\\) . Since \\(C\\) is closed it follows that there is a nonzero vector \\(a\\in\\mathbb{R}^{n}\\) such that \\(a^{\\top}x\\leq 0< a^{\\top}z\\) for all \\(x\\in C\\) . Therefore, \\(a\\in C^{*}\\) and \\(z\\not\\in C^{**}\\) . This is a contradiction. \\(\\lhd\\) Let \\(C=\\{x\\colon Ax\\leq 0\\}=\\{A^{\\top}\\lambda\\colon \\lambda >0\\}^{*}\\) . From the above lemma, it follows that \\(C^{*}=\\{A^{\\top}\\lambda\\colon\\lambda \\geq 0\\}\\) . Minkowski's theorem A finitely constrained cone is finitely generated. \\(\\rhd\\) Let \\(C=\\{x\\colon Ax\\leq 0\\}\\) . Then \\(C^{*}=\\{A^{\\top}\\lambda\\colon\\lambda\\geq 0\\}\\) . From Weyl's theorem it follows that \\(C^{*}=\\{y\\colon By\\leq 0\\}\\) . Since \\(C\\) is a closed cone, it follows that \\[ C=C^{**}=\\{B^{\\top}\\lambda\\colon\\lambda\\geq0\\}.\\quad \\lhd \\]","title":"23 12 07 12 08"},{"location":"23_12_07_12_27/","text":"Here is a proof of the decomposition theorem for polyhedrons. Let us start with definitions. Definition A polyhedron $P\\stackrel{\\mathrm{def}}{=}\\{x\\in\\mathbb{R}^{n}\\colon Ax\\leq b\\}$ is the intersection of a finite number of affine half-spaces. Definition A polytope is the convex hull of a finite set of vectors, i.e. $Q=\\mathrm{hull}(x_{1},x_{2},\\ldots,x_{m})$. Decomposition theorem A set $P\\subset\\mathbb{R}^{n}$ is a polyhedron if and only if there is a polytope $Q$ and a finitely generated cone $C$ such that $P=Q+C$. \\(\\rhd\\) Let \\(P=\\{x\\in\\mathbb{R}^{n}\\colon Ax\\leq b\\}\\) . Consider a finitely constrained cone \\[ C'=\\left\\{\\begin{pmatrix}x\\\\ \\lambda\\end{pmatrix}\\in\\mathbb{R}^{n+1}\\colon \\lambda\\geq 0, Ax-\\lambda b\\leq 0\\right\\}. \\] By Minkowski's theorem , \\(C'\\) is a generated by finitely many vectors \\[ \\begin{pmatrix}x_{1}\\\\ \\lambda_{1}\\end{pmatrix}, \\begin{pmatrix}x_{2}\\\\ \\lambda_{2}\\end{pmatrix}, \\ldots, \\begin{pmatrix}x_{m}\\\\ \\lambda_{m}\\end{pmatrix}. \\] Without loss of generality, we may assume that \\(\\lambda_{i}\\in\\{0,1\\}\\) (otherwise we scale the vectors). Let \\(Q\\) be a convex hull of those \\(x_{i}\\) with \\(\\lambda_{i}=1\\) , and \\(C\\) be the cone generated by \\(x_{i}\\) with \\(\\lambda_{i}=0\\) . Since \\(x\\in P\\) iff \\(\\begin{pmatrix}x\\\\ 1\\end{pmatrix}\\in C'\\) , it follows that \\(x\\in P\\) iff \\[ \\begin{pmatrix}x\\\\ 1\\end{pmatrix}\\in\\mathrm{cone}\\left\\{\\begin{pmatrix}x_{1}\\\\ \\lambda_{1}\\end{pmatrix}, \\begin{pmatrix}x_{2}\\\\ \\lambda_{2}\\end{pmatrix}, \\ldots, \\begin{pmatrix}x_{m}\\\\ \\lambda_{m}\\end{pmatrix}\\right\\}. \\] In other words, \\(x\\in P\\) iff there is \\(\\mu\\geq 0\\) such that \\(x=\\sum\\limits_{i=1}^{m}\\mu_{i}x_{i}\\) and \\(\\sum\\limits_{i=1}^{m}\\mu_{i} \\lambda_{i}=1\\) . Since \\(\\lambda_{i}=1\\) for vertices of \\(Q\\) and \\(\\lambda_{i}=0\\) for defining vectors of \\(C\\) , we have \\(P=Q+C\\) . Let \\(P=Q+C\\) for some polytope \\(P=\\mathrm{hull}(x_{1},\\ldots,x_{m})\\) and cone \\(C=\\mathrm{cone}(y_{1},\\ldots,y_{k})\\) . Then \\(x\\in P\\) iff \\[ \\begin{array}{r} \\begin{pmatrix}x\\\\ 1\\end{pmatrix}\\in C'\\stackrel{\\mathrm{def}}{=}\\mathrm{cone}\\left\\{\\begin{pmatrix}x_{1}\\\\ 1 \\end{pmatrix}, \\ldots, \\begin{pmatrix}x_{m}\\\\ 1\\end{pmatrix},\\right.\\\\ \\left. \\begin{pmatrix}y_{1}\\\\ 0\\end{pmatrix}, \\ldots, \\begin{pmatrix}y_{k}\\\\ 0\\end{pmatrix}\\right\\}. \\end{array} \\] By Weyl's theorem , this cone is finitely constrained, i.e. there are \\(A\\) and \\(b\\) such that \\[ C'=\\left\\{\\begin{pmatrix}x\\\\ \\lambda\\end{pmatrix}\\colon Ax+\\lambda b\\leq 0\\right\\}. \\] Hence, \\(x\\in P\\) iff \\(Ax\\leq -b\\) , i.e. \\(P\\) is a polyhedron. \\(\\lhd\\) Corollary Let $P_{1}$ and $P_{2}$ be polyhedrons. Then there exists $\\min\\limits_{p_{1}\\in P_{1},\\;p_{1}\\in P_{2}}\\|p_{1}-p_{2}\\|$. \\(\\rhd\\) According to the decomposition theorem there are cones \\(C_{1}\\) , \\(C_{2}\\) and polytopes \\(Q_{1}\\) , \\(Q_{2}\\) such that \\(P_{1}=C_{1}+Q_{1}\\) , \\(P_{2}=C_{2}+Q_{2}\\) . Therefore, \\[ \\begin{array}{c} \\min\\limits_{p_{1}\\in P_{1},\\;p_{1}\\in P_{2}}\\|p_{1}-p_{2}\\|=\\\\ =\\min\\limits_{ \\begin{array}{c} c_{1}\\in C_{1},\\;q_{1}\\in Q_{1},\\\\ c_{2}\\in C_{2},\\;q_{2}\\in Q_{2} \\end{array}} \\|(c_{1}-c_{2})-(q_{2}-q_{1})\\|=\\\\ =\\min\\limits_{c\\in C,\\; q\\in Q}\\|c-q\\| \\end{array} \\] where \\(C=C_{1}+(-C_{2})\\) is a finitely generated cone and \\(Q=Q_{1}+(-Q_{2})\\) is a polytope. Since \\(C\\) and \\(Q\\) are closed and \\(Q\\) is bounded, it follows that there exist \\(c^{*}\\in C\\) and \\(q^{*}\\in Q\\) such that \\[ \\|c^{*}-q^{*}\\|=\\min\\limits_{c\\in C,\\; q\\in Q}\\|c-q\\|. \\] By \\(c^{*}\\) and \\(q^{*}\\) we can construct \\(p_{1}^{*}\\in P_{1}\\) and \\(p_{2}^{*}\\in P_{2}\\) such that \\[ \\|p_{1}^{*}-p_{2}^{*}\\|=\\min\\limits_{p_{1}\\in P_{1},\\;p_{2}\\in P_{2}} \\|p_{1}-p_{2}\\|.\\quad\\lhd \\]","title":"23 12 07 12 27"},{"location":"23_12_08_14_54/","text":"Here we discuss the column generation method for solving the one-dimensional cutting stock problem. 1-D Cutting Stock Problem From rolls of length $L$, we need to cut out pieces of length $\\ell_{1}$, $\\ell_{2}$, $\\ldots$, $\\ell_{m}$, respectively, in the quantities $q_{1}$, $q_{2}$, $\\ldots$, $q_{m}$. Our goal is to use the minimum number of rolls. Definition A vector $a=(a_{1}$, $a_{2}$, $\\ldots$, $a_{m})^{\\top}\\in\\mathbb{Z}_{+}^{m}$ is called pattern if $\\sum\\limits_{i=1}^{m}a_{i}\\ell_{i}\\leq L$. If \\(a=(a_{1}\\) , \\(a_{2}\\) , \\(\\ldots\\) , \\(a_{m})^{\\top}\\) is a pattern, then one can simultaneously cut out \\(a_{1}\\) pieces of length \\(\\ell_{1}\\) , \\(a_{2}\\) pieces of length \\(\\ell_{2}\\) , ..., \\(a_{m}\\) pieces of length \\(\\ell_{m}\\) from a roll of length \\(L\\) . Let \\(a^{j}=(a_{1}^{j}\\) , \\(a_{2}^{j}\\) , \\(\\ldots\\) , \\(a_{m}^{j})^{\\top}\\) , \\(j\\in\\{1,\\) \\(2\\) , \\(\\ldots\\) , \\(n\\}\\) , be the set of all possible patterns. We denote by \\(x_{j}\\in\\mathbb{Z}_{+}\\) the number of rolls that need to be cut according to the pattern \\(a^{j}\\) . To find \\(x_{j}\\) we must solve the following IP problem: \\[ \\begin{array}{rl} \\min& \\sum\\limits_{j=1}^{n}x_{j}\\\\ \\mathrm{s.t.}& \\sum\\limits_{j=1}^{n}a_{i}^{j}x_{j}\\geq q_{j},1\\leq i\\leq m\\\\ & x_{j}\\in\\mathbb{Z}_{+},1\\leq j\\leq n \\end{array} \\tag{1} \\] To approximately solve problem (1), we can first solve its relaxation. \\[ \\begin{array}{rl} \\min& \\sum\\limits_{j=1}^{n}x_{j}\\\\ \\mathrm{s.t.}& \\sum\\limits_{j=1}^{n}a_{i}^{j}x_{j}\\geq q_{j},1\\leq i\\leq m\\\\ & x_{j}\\geq 0,1\\leq j\\leq n \\end{array} \\tag{2} \\] and then properly round up an optimal solution of problem (2). However, \\(n\\) (the number of variables) can be very large, so we can't solve problem (2) directly. To solve problem (2) we may use the column generation method. We first select a relatively small subset of patterns \\(a^{1}\\) , \\(a^{2}\\) , \\(\\ldots\\) , \\(a^{k}\\) so that the following truncated LP problem \\[ \\begin{array}{rl} \\min& \\sum\\limits_{j=1}^{k}x_{j}\\\\ \\mathrm{s.t.}& \\sum\\limits_{j=1}^{k}a_{i}^{j}x_{j}\\geq q_{j},1\\leq i\\leq m\\\\ & x_{j}\\geq 0,1\\leq j\\leq k \\end{array} \\tag{3} \\] has a feasible solution. Along with problem (3), let us consider its dual problem \\[ \\begin{array}{rl} \\max& \\sum\\limits_{i=1}^{m}q_{i}y_{i}\\\\ \\mathrm{s.t.}& \\sum\\limits_{i=1}^{m}a_{i}^{j}y_{i}\\leq 1,1\\leq j\\leq k\\\\ & y_{i}\\geq 0,1\\leq i\\leq m \\end{array} \\tag{4} \\] Let \\(x^{\\star}\\) and \\(y^{\\star}\\) be optimal solutions of problem (3) and (4), respectively. If \\[ \\sum\\limits_{i=1}^{m}a_{i}^{j}y_{i}^{\\star}\\leq 1,\\quad j\\in\\{1,2,\\ldots,n\\}, \\tag{5} \\] then \\(y^{\\star}\\) is an optimal solution of the dual problem to problem (2), and then, according to the strong duality theorem , \\(\\begin{pmatrix}x^{*}\\\\\\mathbf{0}\\end{pmatrix}\\in\\mathbb{R}^{n}\\) is an optimal solution of problem (2). To verify inequalities (5), it is sufficient to solve the following problem: \\[ \\begin{array}{rl} \\max& \\sum\\limits_{i=1}^{m}y_{i}^{\\star}z_{i}\\\\ \\mathrm{s.t.}& \\sum\\limits_{i=1}^{m}\\ell_{i}z_{i}\\leq L,1\\leq j\\leq n\\\\ & z_{i}\\in\\mathbb{Z}_{+},1\\leq i\\leq m \\end{array} \\tag{6} \\] Problem (6) can be solved by dynamic programming. Let \\(z^{\\star}\\) be an optimal solution of problem (6). Let us note that \\(z^{\\star}\\) is a pattern. If \\(\\sum\\limits_{i=1}^{m}y_{i}^{\\star}z_{i}^{\\star}> 1\\) , then we add a new column \\(a^{k+1}=z^{\\star}\\) to problem (3) and resolve problems (4) and (6). Otherwise, we stop generating columns and solve problem (3).","title":"23 12 08 14 54"},{"location":"23_12_19_23_14/","text":"Problem Let $S$ be the inscribed sphere of a polyhedron $P$. We call a face of $P$ large if the projection of $S$ onto the plane of the face is entirely inside the face. Prove that there are no more than $6$ large faces. \\(\\rhd\\) By \\(R\\) we denote the radius of \\(S\\) . For each large face we consider a part of \\(S\\) , located in a cone, the vertex of which is the center of \\(S\\) , and the base is the projection of \\(S\\) onto this face. Each such part is a spherical cap of height \\(h=(1-\\sqrt{2}/2)R\\) . Therefore, the area of each part is equal to \\(2\\pi hR=\\pi (2-\\sqrt{2}) R^{2}\\) . Let \\(n\\) be the number of large faces. We have \\(n\\pi (2-\\sqrt{2}) R^{2}\\leq 4\\pi R^{2}\\) , so \\(n\\leq\\dfrac{4}{(2-\\sqrt{2})}<7\\) . Therefore, there are no more than \\(6\\) large faces. \\(\\lhd\\)","title":"23 12 19 23 14"},{"location":"23_12_23_22_39/","text":"Here is an application of a Hamel basis. Theorem 1 Let $P(x)\\in\\mathbb{R}[x]$ be a polynomial of degree $n$. Then there exist periodic functions $g_{1}(x)$, $g_{2}(x)$, $\\ldots$, $g_{n+1}(x)$ such that $P(x)=g_{1}(x)+g_{2}(x)+\\ldots+g_{n+1}(x)$. \\(\\rhd\\) Let \\(A\\) be a Hamel basis. Then every \\(x\\in \\mathbb{R}\\) can be represented as the finite linear combination of elements from \\(A\\) with rational coefficients \\(x=\\sum\\limits_{a\\in A}c_{a}(x)a\\) . For distinct \\(a\\) , \\(b\\in A\\) we have \\(c_{a}(x+b)=c_{a}(x)\\) . In other words, \\(c_{a}(x)\\) is a periodic function with the period \\(b\\in A\\setminus\\{a\\}\\) . Using the identity \\(x=\\sum\\limits_{a\\in A}c_{a}(x)a\\) , we express \\(P(x)\\) as a linear combination of products \\(c_{a}(x)c_{b}(x)\\ldots\\) where \\(a,b,\\ldots\\in A\\) and each product has at most \\(n\\) multipliers. Let \\(a_{0}\\) , \\(a_{1}\\) , \\(\\ldots\\) , \\(a_{n}\\) be \\(n+1\\) distinct elements of \\(A\\) . No product \\(c_{a}(x)c_{b}(x)\\ldots\\) in the expression can involve all of \\(a_{0}\\) , \\(a_{1}\\) , \\(\\ldots\\) , \\(a_{n}\\) . Therefore, we can group all the terms into \\(n+1\\) sums \\(P_{0}(x)\\) , \\(P_{1}(x)\\) , \\(\\ldots\\) , \\(P_{n}(x)\\) so that no term in \\(P_{i}(x)\\) has the multiplier \\(c_{a_{i}}(x)\\) . Thus, we have \\[ P(x)=P_{0}(x)+P_{1}(x)+\\ldots+P_{n}(x), \\] and each \\(P_{i}(x)\\) has period \\(a_{i}\\) . \\(\\lhd\\) Theorem 2 A polynomial of degree $n$ cannot be expressed as a sum of $n$ periodic functions. \\(\\rhd\\) We argue by contradiction. Suppose that for some \\(n\\in\\mathbb{N}\\) , there is a polynomial \\(P(x)\\) of degree \\(n\\) that is the sum of \\(n\\) periodic functions. Let \\(d\\) be a one of the periods. It follows that \\(P(x+d)-P(x)\\) is a polynomial of degree \\(n-1\\) , and it is the sum of \\(n-1\\) periodic functions, since one of the term is canceled. By repeating this, we eventually obtain a polynomial of the first degree that is the sum of one periodic function, which is impossible. \\(\\lhd\\)","title":"23 12 23 22 39"},{"location":"23_12_24_19_28/","text":"Let \\(P=\\{x\\colon Ax\\leq b\\}\\subset\\mathbb{R}^{n}\\) be a nonempty polyhedron. Definition 1 If $c\\in\\mathbb{R}^{n}$ is a nonzero vector for which $d=\\max\\{c^{\\top}x\\colon x\\in P\\}$ is finite, then the set $\\{x\\colon c^{\\top}x=d\\}$ is called a supporting hyperplane of $P$. Definition 2 A face of $P$ is $P$ itself or the intersection of $P$ with a supporting hyperplane of $P$. Theorem A subset $F\\subset P$ of a polyhedron $P=\\{x\\colon Ax\\leq b\\}$ is a face of $P$ if and only if $F=\\{x\\in P\\colon A_{1}x=b_{1}\\}\\neq\\varnothing$ for some subsystem $A_{1}x\\leq b_{1}$ of $Ax\\leq b$. \\(\\rhd\\) Let \\(F=\\{x\\in P\\colon A_{1}x=b_{1}\\}\\) be nonempty. By \\(c\\) we denote the sum of the rows of \\(A_{1}\\) , and by \\(d\\) we denote the sum of the components of \\(b_{1}\\) . Then obviously \\(c^{\\top}x\\leq d\\) for all \\(x\\in P\\) and \\(F=\\{x\\in P\\colon cx=d\\}\\) is a face of \\(P\\) . Let \\(F=\\{x\\in P\\colon cx=d\\}\\) be a face of \\(P\\) . Let \\(A_{1}x\\leq b_{1}\\) be the maximal subsystem of \\(Ax\\leq b\\) such that \\(A_{1}x=b_{1}\\) for all \\(x\\in F\\) . We will show that \\(F=\\{x\\in P\\colon A_{1}x=b_{1}\\}\\) . Assume the converse. Let \\(A_{2}x\\leq b_{2}\\) be the rest of the system \\(Ax\\leq b\\) . Let us choose an arbitrary vector \\(y\\in \\{x\\in P\\colon A_{1}x=b_{1}\\}\\setminus F\\) . We have \\(A_{1}y=b_{1}\\) and \\(c^{\\top}y< c^{\\top}x\\) . Also we choose a vector \\(x\\in F\\) such that \\(A_{1}x=b_{1}\\) and \\(A_{2}x< b_{2}\\) Therefore, for a small enough \\(\\varepsilon > 0\\) and vector \\(z=x+\\varepsilon(x-y)\\) the inequalities \\(A_{2}z<b_{2}\\) hold. Moreover, we have \\(A_{1}z=b_{1}\\) and \\(c^{\\top}z>d\\) , which is impossible. \\(\\lhd\\) Corollary If $F$ be a face of a polyhedron $P=\\{x\\colon Ax\\leq b\\}$, then for some subsystem $A'x\\leq b'$ of $Ax\\leq b$ the set $\\{x\\in\\mathbb{R}^{n}\\colon A'x=b'\\}$ is a nonempty subset of $F$. \\(\\rhd\\) From the previous theorem, it follows that \\(F=\\{x\\in P\\colon A_{1}x=b_{1}\\}\\) for some subsystem \\(A_{1}x\\leq b_{1}\\) of \\(Ax\\leq b\\) . Let us expand the subsystem \\(A_{1}x\\leq b_{1}\\) to a maximal subsystem \\(A'x\\leq b'\\) such that the set \\(\\{x\\in P\\colon A'x=b'\\}\\) is nonempty. Let \\(A''x\\leq b''\\) be the rest of the system \\(Ax\\leq b\\) . It is readily verified that the equality \\(A'x=b'\\) implies the inequality \\(A''x\\leq b''\\) . Therefore, \\(\\{x\\in P\\colon A'x=b'\\}=\\{x\\in\\mathbb{R}^{n} \\colon A'x=b'\\}\\) . \\(\\lhd\\)","title":"23 12 24 19 28"},{"location":"24_01_01_19_34/","text":"Let \\((M,\\rho)\\) be a metric space. Definition Let $A\\subset M$ and $x\\in M$. The value $d(x,A)=\\inf\\limits_{a\\in A}\\rho(x,a)$ is called the distance from $x$ to $A$. Lemma 1 (Urysohn) Let $A$ and $B$ be disjoint closed subsets of $M$. Then there exists a continuous function $f\\colon M\\to[-1,1]$ such that $f(A)=\\{-1\\}$ and $f(B)=\\{1\\}$. \\(\\rhd\\) Since \\(A\\) and \\(B\\) are disjoint, it follows that \\(d(x,A)+d(x,B)>0\\) for each \\(x\\in M\\) . Hence, the function \\[ f(x)\\stackrel{\\mathrm{def}}{=}\\dfrac{d(x,A)-d(x,B)}{d(x,A)+d(x,B)} \\] is well-defined and possesses the desired properties. \\(\\lhd\\) Lemma 2. Let $c>0$, let $X\\subset M$ be a closed subset, and let $f\\colon X\\to[-c,c]$ be a continuous function. Then there exists a continuous function $g\\colon M\\to\\left[-\\dfrac{c}{3},\\dfrac{c}{3}\\right]$ such that $|f(x)-g(x)|\\leq\\dfrac{2c}{3}$ for each $x\\in X$. \\(\\rhd\\) Consider the closed subsets \\(A\\stackrel{\\mathrm{def}}{=} \\{x\\in X\\colon f(x)\\leq-\\dfrac{c}{3}\\}\\) and \\(B\\stackrel{\\mathrm{def}}{=} \\{x\\in X\\colon f(x)\\geq\\dfrac{c}{3}\\}\\) . From Lemma 1, it follows that there exists a continuous function \\(g\\colon M\\to\\left[-\\dfrac{c}{3}, \\dfrac{c}{3}\\right]\\) such that \\(g(A)=\\left\\{-\\dfrac{c}{3}\\right\\}\\) \\(g(B)=\\left\\{\\dfrac{c}{3}\\right\\}\\) . It is easy to see, that \\(|f(x)-g(x)|\\leq \\dfrac{2c}{3}\\) for each \\(x\\in X\\) . \\(\\lhd\\) Theorem (Tietze) Let $X\\subset M$ be a closed subset, let $f\\colon X\\to[-1,1]$ be a continuous function. Then there exists a continuous function $F\\colon M\\to[-1,1]$ such that $F|_{X}\\equiv f$. \\(\\rhd\\) From Lemma 2, it follows that there exists a continuous function \\(g_{0}\\colon X\\to\\left[-\\dfrac{1}{3},\\dfrac{1}{3}\\right]\\) such that \\(|f(x)-g_{0}(x)|\\leq\\dfrac{2}{3}\\) for each \\(x\\in X\\) . Applying Lemma 2 to a function \\(f-g_{0}\\) , we get a continuous function \\(g_{1}\\) such that \\(|g_{1}(x)|\\leq\\dfrac{1}{3}\\left(\\dfrac{2}{3}\\right)\\) for each \\(x\\in M\\) , and \\[ |f(x)-g_{0}(x)-g_{1}(x)|\\leq\\left(\\dfrac{2}{3}\\right)^{2} \\] for each \\(x\\in X\\) . Repeating the arguments, we obtain a sequence \\(g_{0}\\) , \\(g_{1}\\) , \\(g_{2}\\) , \\(\\ldots\\) of continuous function such that \\(|g_{k}(x)|\\leq\\dfrac{1}{3}\\left(\\dfrac{2}{3}\\right)^{k}\\) for each \\(x\\in M\\) , and \\[ |f(x)-g_{0}(x)-g_{1}(x)-\\ldots-g_{k}(x)|\\leq\\left(\\dfrac{2}{3}\\right)^{k+1} \\] for each \\(x\\in X\\) . Denote \\(F(x)\\stackrel{\\mathrm{def}}{=} \\sum\\limits_{k=0}^{+\\infty}g_{k}(x)\\) . Since \\(\\sum\\limits_{k=0}^{+\\infty}\\dfrac{1}{3}\\left(\\dfrac{2}{3}\\right)^{k}=1\\) , it follows that \\(F\\) is continuous, and \\(|F(x)|\\leq 1\\) for each \\(x\\in M\\) . Moreover, \\(F|_{X}\\equiv f\\) . \\(\\lhd\\) Corollary A metric space $M$ is compact if and only if every continuous function $f\\colon M\\to\\mathbb{R}$ is bounded. \\(\\rhd\\) It is well known that if \\(M\\) is compact, then every continuous function \\(f\\colon M\\to\\mathbb{R}\\) is bounded. If \\(M\\) is not compact, then we can choose a sequence \\((x_{n})_{n\\in\\mathbb{N}}\\) in \\(X\\) which has no convergent subsequence. Without loss of generality, we assume that elements of the sequence are pairwise distinct. Every convergent sequence with elements in the set \\(S\\stackrel{\\mathrm{def}}{=}\\{x_{1}\\) , \\(x_{2}\\) , \\(\\ldots\\}\\) must be eventually constant, so it has the limit in \\(S\\) . Hence, the set \\(S\\) is closed, and the function \\(f\\colon S\\to\\mathbb{R}\\) , defined by \\(f(x_{n})=n\\) , is continuous. Using the Tietze extension theorem, we can extend \\(f\\) to a continuous unbounded function \\(F\\colon M\\to\\mathbb{R}\\) . \\(\\lhd\\)","title":"24 01 01 19 34"}]}